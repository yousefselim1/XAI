# -*- coding: utf-8 -*-
"""Model_3_XAI_YousefSelim .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8kkE5nEc3juABeGU6apLkbNqxMVUFK_

## **Load Data**
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("fernando2rad/x-ray-lung-diseases-images-9-classes")

print("Path to dataset files:", path)

import os

# Check available files and directories
os.listdir('/kaggle/input/')

"""## **Preprocessing**"""

# Preprocessing

import cv2
import numpy as np
import os
from tqdm import tqdm

# Function to apply CLAHE and Gamma Correction
def apply_clahe_and_gamma_correction(img):
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # CLAHE
    img_clahe = clahe.apply(img)

    gamma = 1.2  # Adjust this for brightness and contrast
    img_gamma = np.array(255 * (img_clahe / 255) ** gamma, dtype='uint8')  # Gamma correction
    return img_gamma

# Function to preprocess images (resize, CLAHE, Gamma)
def preprocess_images_with_augmentation(source_folder, target_folder, img_size=(224, 224)):
    categories = os.listdir(source_folder)
    for category in categories:
        img_paths = os.listdir(os.path.join(source_folder, category))
        os.makedirs(os.path.join(target_folder, category), exist_ok=True)
        for img_name in tqdm(img_paths, desc=f"Processing {category}"):
            img_path = os.path.join(source_folder, category, img_name)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale
            img_resized = cv2.resize(img, img_size)  # Resize image
            img_preprocessed = apply_clahe_and_gamma_correction(img_resized)
            img_preprocessed = img_preprocessed / 255.0  # Normalize

            # Save preprocessed image

            cv2.imwrite(os.path.join(target_folder, category, img_name), img_preprocessed)

# Usage
source_dir = '/kaggle/input/x-ray-lung-diseases-images-9-classes'
target_dir = '/kaggle/processed/x-ray-lung-diseases-images'
preprocess_images_with_augmentation(source_dir, target_dir)

"""## **split_dataset**"""

##  split_dataset

import os
import shutil
from sklearn.model_selection import train_test_split

def split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15):
    classes = os.listdir(source_dir)

    for class_name in classes:
        class_path = os.path.join(source_dir, class_name)
        images = os.listdir(class_path)
        train_val, test = train_test_split(images, test_size=1 - (train_ratio + val_ratio), random_state=42)
        train, val = train_test_split(train_val, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)

        for split_name, split_data in zip(['train', 'val', 'test'], [train, val, test]):
            split_class_dir = os.path.join(output_dir, split_name, class_name)
            os.makedirs(split_class_dir, exist_ok=True)
            for img in split_data:
                src = os.path.join(class_path, img)
                dst = os.path.join(split_class_dir, img)
                shutil.copy(src, dst)

# Usage:
processed_source = '/kaggle/input/x-ray-lung-diseases-images-9-classes'
processed_output = '/kaggle/working/xray_split'
split_dataset(processed_source, processed_output)

"""## **Set up augmentation**"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set up augmentation for training

# Set up augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,  # Random rotations
    width_shift_range=0.1,  # Horizontal shift
    height_shift_range=0.1,  # Vertical shift
    zoom_range=0.1,  # Zoom in/out
    horizontal_flip=True,  # Random horizontal flip
)

# Validation and test data will only be rescaled (no augmentation)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Set up data generators for train, validation, and test sets
train_generator = train_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'train'),
    target_size=(224, 224),  # Resize all images to 256x256
    color_mode='grayscale',  # Use grayscale images as you mentioned
    class_mode='categorical',  # Multi-class classification
    batch_size=16,
    shuffle=True  # Shuffle to ensure varied batches during training
)

val_generator = val_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'val'),
    target_size=(224, 224),  # Resize images to match input size of model
    color_mode='grayscale',
    class_mode='categorical',
    batch_size=16,
    shuffle=False  # Don't shuffle validation data
)

test_generator = test_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'test'),
    target_size=(224, 224),  # Resize images to match model input size
    color_mode='grayscale',
    class_mode='categorical',
    batch_size=16,
    shuffle=False  # No shuffling for test data
)

"""# **Model 3**

# **DenseNet Model**
"""

from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam

def build_proposed_densenet_model(input_shape=(224, 224, 1), num_classes=9, growth_rate=32):
    model = models.Sequential()

    # First dense block
    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))  # Initial conv layer
    model.add(layers.BatchNormalization())

    # Dense block 1
    for _ in range(3):  # Number of convolutional layers in the dense block
        model.add(layers.Conv2D(growth_rate, (3, 3), activation='relu', padding='same'))  # Add more convolutional layers
        model.add(layers.BatchNormalization())

    # Transition layer 1
    model.add(layers.Conv2D(128, (1, 1), activation='relu', padding='same'))  # Reduce channels with 1x1 conv
    model.add(layers.MaxPooling2D((2, 2)))  # Down-sample with max pooling

    # Dense block 2
    for _ in range(3):
        model.add(layers.Conv2D(growth_rate, (3, 3), activation='relu', padding='same'))
        model.add(layers.BatchNormalization())

    # Transition layer 2
    model.add(layers.Conv2D(256, (1, 1), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))  # Down-sample with max pooling

    # Flatten and fully connected layers
    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(num_classes, activation='softmax'))  # For multi-class classification

    optimizer = Adam(learning_rate=0.001)  #  learning rate

    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Build the proposed DenseNet-like model
model = build_proposed_densenet_model(input_shape=(224, 224, 1))
model.summary()

"""## **Training the Model**"""

import tensorflow as tf

# Enable memory growth for GPUs
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

"""## **Fitting the model**"""

from tensorflow.keras.callbacks import EarlyStopping

# Set up early stopping based on validation loss
early_stopping = EarlyStopping(
    monitor='val_loss',   # Monitor validation loss
    patience=10,          # Stop after 10 epochs without improvement
)

from tensorflow.keras import mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Train the model

history = model.fit(
    train_generator,
    epochs=30,
    validation_data=val_generator,
    batch_size=16,
    callbacks=[early_stopping]
)

"""## **Evaluation**"""

# Evaluation

test_loss, test_acc = model.evaluate(test_generator)
print(f"Test accuracy: {test_acc}")

"""## **Classification report**"""

# Classification report

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Predict on the test data
y_true = test_generator.classes  # True labels
y_pred = model.predict(test_generator)  # Predicted labels
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# Classification report
print("Classification Report:\n", classification_report(y_true, y_pred_classes))

"""## **Confusion matrix**"""

# Confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
print("Confusion Matrix:\n", cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""## **Plot training and validation accuracy and loss**"""

# Plot training and validation accuracy and loss
plt.figure(figsize=(12, 6))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""# ***Interpetability***

## **lime**
"""

!pip install lime

import numpy as np
import matplotlib.pyplot as plt
from lime.lime_image import LimeImageExplainer
from skimage.segmentation import mark_boundaries
import tensorflow as tf


def predict_fn(images_rgb):
    # Convert the images from RGB to grayscale using the luminance formula
    images_gray = np.dot(images_rgb[..., :3], [0.2989, 0.5870, 0.1140])  # RGB -> Grayscale
    images_gray = np.expand_dims(images_gray, axis=-1)  # Add a channel dimension for grayscale
    return model.predict(images_gray)

# Initialize LIME explainer
explainer = LimeImageExplainer()

# Get a batch of test images
images, labels = next(test_generator)
images = images[:5]  # Take the first 5 images for explanation

# Prepare the plot for displaying images and explanations
fig, axes = plt.subplots(len(images), 2, figsize=(10, 5 * len(images)))

for i, img_gray in enumerate(images):
    # Convert grayscale image to fake RGB format for LIME (3 channels)
    img_rgb = np.repeat(img_gray, 3, axis=2)  # Convert grayscale to RGB format


    explanation = explainer.explain_instance(

        img_rgb,  # Input image (fake RGB)
        predict_fn,  # The prediction function that returns model output
        top_labels=1,  # Top 1 label to explain
        hide_color=0,  # Don't hide the original image's color (for the explanation)
        num_samples=500  # Number of perturbed samples to generate for explanation
    )


    # Get the explanation for the top predicted class
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,  # Focus on positive contributions
        num_features=5,  # Display the top 5 features (superpixels)
        hide_rest=False  # Don't hide other parts of the image
    )


    # Plot original grayscale image
    axes[i, 0].imshow(img_gray.squeeze(), cmap='gray')  # Squeeze to remove extra dimension
    axes[i, 0].set_title("Original Image")
    axes[i, 0].axis('off')  # Hide axis for cleaner display

    # Plot the LIME explanation (superpixels with boundaries)
    axes[i, 1].imshow(mark_boundaries(temp, mask))  # Overlay the explanation with boundaries
    axes[i, 1].set_title("LIME Explanation")
    axes[i, 1].axis('off')  # Hide axis for cleaner display

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""## **Shap**"""

import os
import numpy as np
import tensorflow as tf
import shap
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt

# ========== MEMORY AND GPU CONFIGURATION ==========
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
tf.keras.backend.clear_session()


# ========== MODEL INPUT VERIFICATION ==========
MODEL_INPUT_SIZE = model.input_shape[1:3]  # (224, 224) shape
print(f"Model requires input shape: (None, {MODEL_INPUT_SIZE[0]}, {MODEL_INPUT_SIZE[1]}, 1)")


# ========== DATA PREPARATION ==========
def preprocess_image(img_array):
    """Preprocess image for SHAP"""
    img_array = img_array / 255.0  # Normalize to [0, 1]
    return np.expand_dims(img_array, axis=0)  # Shape: (1, 224, 224, 1)



# Get a batch of images from the test_generator
test_images, test_labels = next(test_generator)


# Choose the first image from the batch
img_array = test_images[0]  # Take the first image (grayscale)



# Preprocess the image for SHAP
img_array = preprocess_image(img_array)



# Prepare background data (subset of test data)
background, _ = next(test_generator)
background = tf.image.resize(background, MODEL_INPUT_SIZE).numpy()[:5]  # Use only 5 samples for background
print("Verified background shape:", background.shape)



# ========== SHAP COMPUTATION ==========
def compute_shap_values():
    """Compute SHAP values"""
    try:
        # Use CPU if GPU is out of memory
        with tf.device('/CPU:0'):
            explainer = shap.DeepExplainer(model, background)
            return explainer.shap_values(img_array)
    except Exception as e:
        print(f"DeepExplainer failed: {str(e)}")
        return None

shap_values = compute_shap_values()

if shap_values is None:
    raise RuntimeError("SHAP computation failed")



# ========== STANDARD SHAP VISUALIZATION ==========

shap.image_plot(shap_values, img_array)

"""## **Saliency Map**"""

import matplotlib.pyplot as plt
import tensorflow as tf

def plot_saliency_map(model, img_array):
    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(img_tensor)
        preds = model(img_tensor)
        top_class = preds[:, tf.argmax(preds[0])]

    grads = tape.gradient(top_class, img_tensor)
    saliency = tf.reduce_max(tf.abs(grads), axis=-1)[0]

    plt.imshow(saliency, cmap='hot')
    plt.title('Saliency Map')
    plt.axis('off')
    plt.show()

# Get a single image from the test generator
img, _ = next(test_generator)
img = img[0:1]  # Pick 1 sample

plot_saliency_map(model, img)

"""## **Integrated Gradients**"""

import numpy as np
import matplotlib.pyplot as plt

# function integrated_gradients

def integrated_gradients(model, img_array, baseline=None, steps=50):
    if baseline is None:
        baseline = np.zeros(img_array.shape).astype(np.float32)

    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)
    baseline_tensor = tf.convert_to_tensor(baseline, dtype=tf.float32)

    interpolated_images = [baseline_tensor + (float(i)/steps)*(img_tensor-baseline_tensor) for i in range(steps + 1)]
    grads = []
    for img in interpolated_images:
        with tf.GradientTape() as tape:
            tape.watch(img)
            pred = model(img)
            top_class = pred[:, tf.argmax(pred[0])]
        grad = tape.gradient(top_class, img)
        grads.append(grad.numpy())

    avg_grads = np.mean(grads, axis=0)
    integrated_grads = (img_tensor - baseline_tensor) * avg_grads
    return integrated_grads.numpy()

# Get a single image from the test generator
img, _ = next(test_generator)
img = img[0:1]

# Compute Integrated Gradients
int_grads = integrated_gradients(model, img)

# Plot
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Original image
ax[0].imshow(img.squeeze(), cmap='gray')
ax[0].set_title('Original Image')
ax[0].axis('off')

# Integrated Gradients overlay
ax[1].imshow(img.squeeze(), cmap='gray', alpha=0.6)
ax[1].imshow(np.abs(int_grads.squeeze()), cmap='hot', alpha=0.4)
ax[1].set_title('Integrated Gradients')
ax[1].axis('off')

plt.tight_layout()
plt.show()

"""# **Comarison with pretrained model**

# **Load EfficientNetB0 with Pretrained Weights**

## **Intialize the Model**
"""

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

def load_efficientnet_model(input_shape=(224, 224, 3), num_classes=9):
    # Load EfficientNetB0 with pretrained weights
    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)

    # Unfreeze the last few layers of EfficientNetB0
    for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers for fine-tuning
        layer.trainable = True

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')  # Output layer for multi-class classification
    ])

    # Compile the model with a lower learning rate for fine-tuning
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Initialize the EfficientNet model with enhanced fine-tuning
model = load_efficientnet_model(input_shape=(224, 224, 3), num_classes=9)

# Display model summary
model.summary()

import tensorflow as tf

# Train the EfficientNetB0 model with your dataset
history = model.fit(
    train_generator,  #  train_generator
    epochs=10,  # Adjust epochs
    validation_data=val_generator,  # val_generator
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]
)

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(test_generator)  # test_generator
print(f"Test Accuracy: {test_acc}")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get the predicted labels
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print the classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# **Interpetability**

## **Lime**
"""

!pip install lime

import numpy as np
import matplotlib.pyplot as plt
from lime.lime_image import LimeImageExplainer
from skimage.segmentation import mark_boundaries
import tensorflow as tf

# Define the correct prediction function for LIME
def predict_fn(images_rgb):
    # Convert the images from RGB to grayscale using the luminance formula
    images_gray = np.dot(images_rgb[..., :3], [0.2989, 0.5870, 0.1140])  # RGB -> Grayscale
    images_gray = np.expand_dims(images_gray, axis=-1)  # Add a channel dimension for grayscale
    return model.predict(images_gray)

# Initialize LIME explainer
explainer = LimeImageExplainer()

# Get a batch of test images
images, labels = next(test_generator)
images = images[:5]  # Take the first 5 images for explanation

# Prepare the plot for displaying images and explanations
fig, axes = plt.subplots(len(images), 2, figsize=(10, 5 * len(images)))

for i, img_gray in enumerate(images):
    # Convert grayscale image to fake RGB format for LIME (3 channels)
    img_rgb = np.repeat(img_gray, 3, axis=2)  # Convert grayscale to RGB format

    # Explain the image using LIME
    explanation = explainer.explain_instance(
        img_rgb,  # Input image (fake RGB)
        predict_fn,  # The prediction function that returns model output
        top_labels=1,  # Top 1 label to explain
        hide_color=0,  # Don't hide the original image's color (for the explanation)
        num_samples=500  # Number of perturbed samples to generate for explanation
    )

    # Get the explanation for the top predicted class
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,  # Focus on positive contributions
        num_features=5,  # Display the top 5 features (superpixels)
        hide_rest=False  # Don't hide other parts of the image
    )


    # Plot original grayscale image
    axes[i, 0].imshow(img_gray.squeeze(), cmap='gray')  # Squeeze to remove extra dimension
    axes[i, 0].set_title("Original Image")
    axes[i, 0].axis('off')  # Hide axis for cleaner display

    # Plot the LIME explanation (superpixels with boundaries)
    axes[i, 1].imshow(mark_boundaries(temp, mask))  # Overlay the explanation with boundaries
    axes[i, 1].set_title("LIME Explanation")
    axes[i, 1].axis('off')  # Hide axis for cleaner display

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# **Inception V3**"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import os

# Load InceptionV3 with pretrained weights
def load_inceptionv3_model(input_shape=(224, 224, 3), num_classes=9):
    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)

    # Unfreeze the last few layers of InceptionV3 for fine-tuning
    for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers for fine-tuning
        layer.trainable = True

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')  # Output layer for multi-class classification
    ])

    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Set up augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,  # Random rotations
    width_shift_range=0.1,  # Horizontal shift
    height_shift_range=0.1,  # Vertical shift
    zoom_range=0.1,  # Zoom in/out
    horizontal_flip=True  # Random horizontal flip
)

# Validation and test data will only be rescaled (no augmentation)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Set up data generators for train, validation, and test sets
train_generator = train_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'train'),
    target_size=(224, 224),  # Resize all images to 224x224
    color_mode='rgb',  # Load images as RGB (even though they are grayscale, will convert to RGB)
    class_mode='categorical',  # Multi-class classification
    batch_size=16,
    shuffle=True  # Shuffle to ensure varied batches during training
)

val_generator = val_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'val'),
    target_size=(224, 224),  # Resize images to match input size of model
    color_mode='rgb',  # Load images as RGB (even though they are grayscale, will convert to RGB)
    class_mode='categorical',
    batch_size=16,
    shuffle=False  # Don't shuffle validation data
)

test_generator = test_datagen.flow_from_directory(
    directory=os.path.join(processed_output, 'test'),
    target_size=(224, 224),  # Resize images to match model input size
    color_mode='rgb',  # Load images as RGB (even though they are grayscale, will convert to RGB)
    class_mode='categorical',
    batch_size=16,
    shuffle=False  # No shuffling for test data
)

# Initialize the InceptionV3 model with enhanced fine-tuning
model = load_inceptionv3_model(input_shape=(224, 224, 3), num_classes=9)

# Display model summary
model.summary()

# Train the model with your dataset
history = model.fit(
    train_generator,  # train_generator
    epochs=10,  # Adjust epochs
    validation_data=val_generator,  # val_generator
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]
)

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(test_generator)  # test_generator
print(f"Test Accuracy: {test_acc}")

import matplotlib.pyplot as plt

# Function to plot accuracy and loss for both models
def plot_history(history):
    # Plot training & validation accuracy values
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot training & validation loss values
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()


plot_history(history)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Get the predicted labels
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print the classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# **Interpetability**

## **Lime**
"""

import numpy as np
import matplotlib.pyplot as plt
from lime.lime_image import LimeImageExplainer
from skimage.segmentation import mark_boundaries
import tensorflow as tf

# Define the correct prediction function for LIME using your pre-trained InceptionV3 model
def predict_fn(images_rgb):
    # Preprocess the images for InceptionV3 (resize and normalize)

    # Process each image individually
    processed_images = []
    for img in images_rgb:
        img_resized = tf.image.resize(img[np.newaxis, ...], (224, 224))
        img_resized = tf.keras.applications.inception_v3.preprocess_input(img_resized)
        processed_images.append(img_resized)

    processed_images = np.vstack(processed_images)  # Stack the processed images

    return model.predict(processed_images)  # Predict on the stack


# Initialize LIME explainer
explainer = LimeImageExplainer()

# Get a batch of test images
images, labels = next(test_generator)
images = images[:5]  # Take the first 5 images for explanation

# Prepare the plot for displaying images and explanations
fig, axes = plt.subplots(len(images), 2, figsize=(10, 5 * len(images)))

for i, img_gray in enumerate(images):
    # Convert grayscale image to fake RGB format for LIME (3 channels)

    img_rgb = np.stack((img_gray[:,:,0],)*3, axis=-1)

    # Explain the image using LIME
    explanation = explainer.explain_instance(
        img_rgb,  # Input image (fake RGB)
        predict_fn,  # The prediction function that returns model output
        top_labels=1,  # Top 1 label to explain
        hide_color=0,  # Don't hide the original image's color (for the explanation)
        num_samples=500  # Number of perturbed samples to generate for explanation
    )

    # Get the explanation for the top predicted class
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,  # Focus on positive contributions
        num_features=5,  # Display the top 5 features (superpixels)
        hide_rest=False  # Don't hide other parts of the image
    )

    # Plot original grayscale image
    axes[i, 0].imshow(img_gray.squeeze(), cmap='gray')  # Squeeze to remove extra dimension
    axes[i, 0].set_title("Original Image")
    axes[i, 0].axis('off')  # Hide axis for cleaner display

    # Plot the LIME explanation (superpixels with boundaries)
    axes[i, 1].imshow(mark_boundaries(temp, mask))  # Overlay the explanation with boundaries
    axes[i, 1].set_title("LIME Explanation")
    axes[i, 1].axis('off')

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""## **Integrated Gradients**"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# Function for Integrated Gradients
def integrated_gradients(model, img_array, baseline=None, steps=50):
    if baseline is None:
        baseline = np.zeros(img_array.shape).astype(np.float32)

    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)
    baseline_tensor = tf.convert_to_tensor(baseline, dtype=tf.float32)

    interpolated_images = [baseline_tensor + (float(i)/steps)*(img_tensor-baseline_tensor) for i in range(steps + 1)]
    grads = []
    for img in interpolated_images:
        with tf.GradientTape() as tape:
            tape.watch(img)
            pred = model(img)
            top_class = pred[:, tf.argmax(pred[0])]  # Get top predicted class
        grad = tape.gradient(top_class, img)
        grads.append(grad.numpy())

    avg_grads = np.mean(grads, axis=0)
    integrated_grads = (img_tensor - baseline_tensor) * avg_grads
    return integrated_grads.numpy()

# Get a single image from the test generator
img, _ = next(test_generator)
img = img[0:1]  # Get the first image from the batch

# Compute Integrated Gradients
int_grads = integrated_gradients(model, img)

# Plot the results with improved heatmap visualization
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Original image
ax[0].imshow(img.squeeze(), cmap='gray')
ax[0].set_title('Original Image')
ax[0].axis('off')

# Improved Integrated Gradients overlay with better transparency
ax[1].imshow(img.squeeze(), cmap='gray', alpha=0.6)
ax[1].imshow(np.abs(int_grads.squeeze()), cmap='hot', alpha=0.6)  # Adjust alpha for better visibility
ax[1].set_title('Integrated Gradients')
ax[1].axis('off')

plt.tight_layout()
plt.show()