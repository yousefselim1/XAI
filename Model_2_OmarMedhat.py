# -*- coding: utf-8 -*-
"""paper_six.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16eng2xTGGnBbbM7f7vfRSw9VYoYhIQlB
"""

import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import tensorflow as tf
import kagglehub

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

path = kagglehub.dataset_download("fernando2rad/x-ray-lung-diseases-images-9-classes")
print("Path to dataset files:", path)

def split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15):
    classes = os.listdir(source_dir)
    for class_name in classes:
        class_path = os.path.join(source_dir, class_name)
        images = os.listdir(class_path)
        train_val, test = train_test_split(images, test_size=1 - (train_ratio + val_ratio), random_state=42)
        train, val = train_test_split(train_val, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)

        for split_name, split_data in zip(['train', 'val', 'test'], [train, val, test]):
            split_class_dir = os.path.join(output_dir, split_name, class_name)
            os.makedirs(split_class_dir, exist_ok=True)
            for img in split_data:
                shutil.copy(os.path.join(class_path, img), os.path.join(split_class_dir, img))

output_path = '/kaggle/working/xray_split'
split_dataset(path, output_path)

IMG_SIZE = 380
BATCH_SIZE = 16

train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=15,
                                   width_shift_range=0.1, height_shift_range=0.1,
                                   zoom_range=0.1, horizontal_flip=True)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(os.path.join(output_path, 'train'),
    target_size=(IMG_SIZE, IMG_SIZE), class_mode='categorical', color_mode='rgb', batch_size=BATCH_SIZE)
val_gen = val_datagen.flow_from_directory(os.path.join(output_path, 'val'),
    target_size=(IMG_SIZE, IMG_SIZE), class_mode='categorical', color_mode='rgb', batch_size=BATCH_SIZE)
test_gen = test_datagen.flow_from_directory(os.path.join(output_path, 'test'),
    target_size=(IMG_SIZE, IMG_SIZE), class_mode='categorical', color_mode='rgb', batch_size=BATCH_SIZE, shuffle=False)

base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
x = GlobalAveragePooling2D()(base_model.output)
x = Dropout(0.4)(x)
output = Dense(train_gen.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

checkpoint = ModelCheckpoint("efficientnet_lung_model.h5", save_best_only=True, monitor='val_accuracy')
early_stop = EarlyStopping(patience=5, restore_best_weights=True)

model.fit(train_gen, validation_data=val_gen, epochs=10, callbacks=[checkpoint, early_stop])

def make_gradcam_heatmap(img_array, model, last_conv_layer_name='top_conv'):
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        class_channel = predictions[:, tf.argmax(predictions[0])]
    grads = tape.gradient(class_channel, conv_outputs)[0]
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))
    conv_outputs = conv_outputs[0]
    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)
    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def display_gradcam(img_path):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img_array = np.expand_dims(img, axis=0) / 255.0
    heatmap = make_gradcam_heatmap(img_array, model)
    heatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap * 0.4 + img
    plt.imshow(cv2.cvtColor(superimposed.astype('uint8'), cv2.COLOR_BGR2RGB))
    plt.title("Grad-CAM")
    plt.axis('off')
    plt.show()

# Example usage
sample_img_path = test_gen.filepaths[0]
display_gradcam(sample_img_path)

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries

lime_explainer = lime_image.LimeImageExplainer()

def explain_lime(img_path):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0
    img_array = np.expand_dims(img, axis=0)
    def predict_fn(x): return model.predict(x)
    explanation = lime_explainer.explain_instance(img.astype('double'), predict_fn, top_labels=1, hide_color=0, num_samples=1000)
    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
    plt.imshow(mark_boundaries(temp / 255.0, mask))
    plt.title("LIME Explanation")
    plt.axis('off')
    plt.show()

# Example usage
explain_lime(sample_img_path)

X_surrogate = []
y_pred = []

for _ in range(5):
    x_batch, _ = next(test_gen)
    preds = model.predict(x_batch)
    X_surrogate.extend(x_batch.reshape(len(x_batch), -1))
    y_pred.extend(np.argmax(preds, axis=1))

surrogate_model = DecisionTreeClassifier(max_depth=5)
surrogate_model.fit(X_surrogate, y_pred)

print("Surrogate model accuracy (approximation):", accuracy_score(y_pred, surrogate_model.predict(X_surrogate)))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# Extract features from the pre-trained model (EfficientNetB4)
def extract_features(generator, model):
    features = []
    labels = []
    for data, label in generator:
        # Get the features from the global average pooling layer
        features_batch = model.predict(data)
        features.append(features_batch)
        labels.append(label)

        # Break after going through one batch to avoid memory issues (or adjust as needed)
        break

    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    return features, labels

# Extract features for the training and test sets
train_features, train_labels = extract_features(train_gen, model)
test_features, test_labels = extract_features(test_gen, model)

# Flatten the features for the decision tree model (optional step depending on the surrogate model)
train_features_flat = train_features.reshape(train_features.shape[0], -1)
test_features_flat = test_features.reshape(test_features.shape[0], -1)

# Train a decision tree classifier as a surrogate model
surrogate_model = DecisionTreeClassifier(max_depth=5)
surrogate_model.fit(train_features_flat, np.argmax(train_labels, axis=1))  # Use argmax for multi-class

# Evaluate the surrogate model
train_preds = surrogate_model.predict(train_features_flat)
test_preds = surrogate_model.predict(test_features_flat)

train_accuracy = accuracy_score(np.argmax(train_labels, axis=1), train_preds)
test_accuracy = accuracy_score(np.argmax(test_labels, axis=1), test_preds)

print("Surrogate Model Accuracy on Train Data:", train_accuracy)
print("Surrogate Model Accuracy on Test Data:", test_accuracy)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(surrogate_model, filled=True, feature_names=[f"Feature {i}" for i in range(train_features_flat.shape[1])], class_names=[str(i) for i in range(train_labels.shape[1])])
plt.show()